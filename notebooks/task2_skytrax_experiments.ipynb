{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGwvq6C2YYNZ",
        "outputId": "ae464fe6-4a17-45db-eb81-32fb8aacaabf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping complete! Data saved to 'data/british_airways_reviews.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# URL of British Airways reviews on Skytrax\n",
        "BASE_URL = \"https://www.airlinequality.com/airline-reviews/british-airways/page/{}/\"\n",
        "\n",
        "# Number of pages to scrape (adjust as needed)\n",
        "NUM_PAGES = 5\n",
        "\n",
        "# List to store scraped data\n",
        "reviews_list = []\n",
        "\n",
        "# Function to scrape reviews\n",
        "def scrape_reviews():\n",
        "    for page in range(1, NUM_PAGES + 1):\n",
        "        url = BASE_URL.format(page)\n",
        "        print(f\"Scraping page {page}...\")\n",
        "\n",
        "        # Simulate a real user request\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find all review containers\n",
        "        reviews = soup.find_all(\"article\", class_=\"comp_reviews-review\")\n",
        "\n",
        "        for review in reviews:\n",
        "            try:\n",
        "                title = review.find(\"span\", itemprop=\"author\").text.strip()\n",
        "                rating = review.find(\"div\", class_=\"reviewRating\").get(\"class\")[1].split(\"-\")[-1]  # Extract rating\n",
        "                date = review.find(\"time\", itemprop=\"datePublished\").text.strip()\n",
        "                content = review.find(\"div\", class_=\"text_content\").text.strip()\n",
        "\n",
        "                # Store in list\n",
        "                reviews_list.append([title, rating, date, content])\n",
        "\n",
        "            except AttributeError:\n",
        "                continue  # Skip if any field is missing\n",
        "\n",
        "        # Delay to avoid bot detection\n",
        "        time.sleep(2)\n",
        "\n",
        "# Run scraping function\n",
        "scrape_reviews()\n",
        "\n",
        "# Save data to CSV\n",
        "import os\n",
        "\n",
        "# Create the \"data\" directory if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "# Save data to CSV\n",
        "df = pd.DataFrame(reviews_list, columns=[\"Title\", \"Rating\", \"Date\", \"Review\"])\n",
        "df.to_csv(\"data/british_airways_reviews.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to 'data/british_airways_reviews.csv'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Load original raw data\n",
        "df = pd.read_csv(\"data/british_airways_reviews.csv\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"Cleaned_Review\"] = df[\"Review\"].apply(clean_text)\n",
        "\n",
        "# Drop empty reviews just in case\n",
        "df = df[df[\"Cleaned_Review\"].str.strip().astype(bool)]\n",
        "\n",
        "# Save cleaned file\n",
        "df.to_csv(\"data/cleaned_british_airways_reviews.csv\", index=False)\n",
        "print(\"✅ Cleaned reviews ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilp0bvRU8T6g",
        "outputId": "7dc1f04e-4961-471c-9180-b3f4aee4c219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned reviews ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "# Load cleaned dataset\n",
        "df = pd.read_csv(\"data/cleaned_british_airways_reviews.csv\")\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply sentiment analysis to the cleaned reviews\n",
        "df[\"Sentiment_Score\"] = df[\"Cleaned_Review\"].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n",
        "\n",
        "# Categorize sentiment\n",
        "def get_sentiment(score):\n",
        "    if score >= 0.05:\n",
        "        return \"Positive\"\n",
        "    elif score <= -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "df[\"Sentiment_Label\"] = df[\"Sentiment_Score\"].apply(get_sentiment)\n",
        "\n",
        "# Save the result\n",
        "df.to_csv(\"data/sentiment_british_airways_reviews.csv\", index=False)\n",
        "print(\"Sentiment analysis complete! Results saved to 'data/sentiment_british_airways_reviews.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FctRrKjvLC4v",
        "outputId": "95551aaf-70bf-4ada-e4cd-9735757ceab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis complete! Results saved to 'data/sentiment_british_airways_reviews.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load cleaned dataset\n",
        "df = pd.read_csv(\"data/cleaned_british_airways_reviews.csv\")\n",
        "\n",
        "# Combine all reviews into one big string\n",
        "text = \" \".join(df[\"Cleaned_Review\"].dropna().astype(str).tolist())\n",
        "\n",
        "# Create the word cloud\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "# Display it\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Frequent Words in British Airways Reviews\", fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "KwhBF5EoPnC9",
        "outputId": "03edc65d-5cbd-4b4a-ea6c-4e6d3df80d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "We need at least 1 word to plot a word cloud, got 0.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7dbeee84941a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Display it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \"\"\"\n\u001b[1;32m    623\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    411\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    412\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"data/cleaned_british_airways_reviews.csv\")\n",
        "print(df[\"Cleaned_Review\"].head(10))\n",
        "print(\"Total non-empty reviews:\", df[\"Cleaned_Review\"].dropna().str.strip().astype(bool).sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka1tUwSRP8Di",
        "outputId": "47de5d47-f7ee-44de-cf9d-39d18bf8d2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Series([], Name: Cleaned_Review, dtype: object)\n",
            "Total non-empty reviews: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_raw = pd.read_csv(\"data/british_airways_reviews.csv\")\n",
        "print(\"Sample raw reviews:\\n\", df_raw[\"Review\"].dropna().head(5))\n",
        "print(\"Total reviews:\", len(df_raw))\n",
        "print(\"Missing review text:\", df_raw[\"Review\"].isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1okFuB6oQTq_",
        "outputId": "880d3a7f-06c9-43af-96f4-f47b70e2182b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample raw reviews:\n",
            " Series([], Name: Review, dtype: object)\n",
            "Total reviews: 0\n",
            "Missing review text: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load raw data\n",
        "df_raw = pd.read_csv(\"data/british_airways_reviews.csv\")\n",
        "\n",
        "# Inspect raw reviews\n",
        "print(\"Sample Raw Reviews:\\n\", df_raw[\"Review\"].head(10))\n",
        "print(\"Missing Reviews:\", df_raw[\"Review\"].isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beGZ6gguR8HJ",
        "outputId": "52d6dd4f-0d6f-4ab1-ebba-e12ada295341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Raw Reviews:\n",
            " Series([], Name: Review, dtype: object)\n",
            "Missing Reviews: 0\n"
          ]
        }
      ]
    }
  ]
}